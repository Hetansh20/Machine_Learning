{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Z-Y96p4PQ64q",
        "outputId": "be367473-8346-442a-a736-22acbf3c6b29"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Nutrition_Physical_Activityand_Obesity-_Behavioral_Risk_Factor_Surveillance_System.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3869775518.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Nutrition_Physical_Activityand_Obesity-_Behavioral_Risk_Factor_Surveillance_System.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Data Understanding & Exploration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Nutrition_Physical_Activityand_Obesity-_Behavioral_Risk_Factor_Surveillance_System.csv'"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from sklearn.impute import SimpleImputer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('Nutrition_Physical_Activityand_Obesity-_Behavioral_Risk_Factor_Surveillance_System.csv')\n",
        "\n",
        "# Data Understanding & Exploration\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nData Types:\")\n",
        "print(df.dtypes)\n",
        "print(\"\\nMissing Values:\")\n",
        "print(df.isnull().sum())\n",
        "print(\"\\nBasic Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Data Preprocessing\n",
        "# Filter relevant data for regression analysis\n",
        "# Let's focus on obesity rates as the target variable\n",
        "obesity_data = df[df['Question'].str.contains('obesity', case=False, na=False)].copy()\n",
        "\n",
        "print(f\"Obesity data shape: {obesity_data.shape}\")\n",
        "\n",
        "# Handle missing values in Data_Value\n",
        "obesity_data = obesity_data[obesity_data['Data_Value'].notna()]\n",
        "\n",
        "# Convert categorical variables to numeric\n",
        "categorical_cols = ['LocationAbbr', 'Gender', 'Education', 'Income', 'Race/Ethnicity', 'Stratification1']\n",
        "\n",
        "# Create a copy for encoding\n",
        "df_encoded = obesity_data.copy()\n",
        "\n",
        "# Encode categorical variables\n",
        "le = LabelEncoder()\n",
        "for col in categorical_cols:\n",
        "    if col in df_encoded.columns:\n",
        "        df_encoded[col] = df_encoded[col].astype(str)\n",
        "        df_encoded[col] = le.fit_transform(df_encoded[col])\n",
        "\n",
        "# Select features and target\n",
        "features = ['YearStart', 'LocationAbbr', 'Sample_Size', 'Gender', 'Education', 'Income', 'Race/Ethnicity']\n",
        "target = 'Data_Value'\n",
        "\n",
        "# Filter only rows where all selected features are available\n",
        "df_filtered = df_encoded[features + [target]].dropna()\n",
        "\n",
        "X = df_filtered[features]\n",
        "y = df_filtered[target]\n",
        "\n",
        "print(f\"Final dataset shape: {X.shape}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Model Implementation - Five Different Regression Techniques\n",
        "\n",
        "# 1. Linear Regression\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"1. LINEAR REGRESSION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "y_pred_lr = lr.predict(X_test_scaled)\n",
        "\n",
        "# Cross-validation\n",
        "lr_cv_scores = cross_val_score(lr, X_train_scaled, y_train, cv=5, scoring='r2')\n",
        "\n",
        "# 2. Ridge Regression\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"2. RIDGE REGRESSION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "ridge = Ridge(alpha=1.0)\n",
        "ridge.fit(X_train_scaled, y_train)\n",
        "y_pred_ridge = ridge.predict(X_test_scaled)\n",
        "ridge_cv_scores = cross_val_score(ridge, X_train_scaled, y_train, cv=5, scoring='r2')\n",
        "\n",
        "# 3. Random Forest Regression\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"3. RANDOM FOREST REGRESSION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "rf_cv_scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='r2')\n",
        "\n",
        "# 4. Gradient Boosting Regression\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"4. GRADIENT BOOSTING REGRESSION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "gb = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "y_pred_gb = gb.predict(X_test)\n",
        "gb_cv_scores = cross_val_score(gb, X_train, y_train, cv=5, scoring='r2')\n",
        "\n",
        "# 5. Support Vector Regression\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"5. SUPPORT VECTOR REGRESSION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
        "svr.fit(X_train_scaled, y_train)\n",
        "y_pred_svr = svr.predict(X_test_scaled)\n",
        "svr_cv_scores = cross_val_score(svr, X_train_scaled, y_train, cv=5, scoring='r2')\n",
        "\n",
        "# Performance Evaluation\n",
        "def evaluate_model(y_true, y_pred, model_name, cv_scores):\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"{model_name} Performance:\")\n",
        "    print(f\"MSE: {mse:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "    print(f\"Cross-Validation R²: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    return {\n",
        "        'Model': model_name,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'MAE': mae,\n",
        "        'R2': r2,\n",
        "        'CV_R2_Mean': cv_scores.mean(),\n",
        "        'CV_R2_Std': cv_scores.std()\n",
        "    }\n",
        "\n",
        "# Evaluate all models\n",
        "results = []\n",
        "\n",
        "results.append(evaluate_model(y_test, y_pred_lr, \"Linear Regression\", lr_cv_scores))\n",
        "results.append(evaluate_model(y_test, y_pred_ridge, \"Ridge Regression\", ridge_cv_scores))\n",
        "results.append(evaluate_model(y_test, y_pred_rf, \"Random Forest\", rf_cv_scores))\n",
        "results.append(evaluate_model(y_test, y_pred_gb, \"Gradient Boosting\", gb_cv_scores))\n",
        "results.append(evaluate_model(y_test, y_pred_svr, \"Support Vector Regression\", svr_cv_scores))\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"COMPARISON OF ALL MODELS\")\n",
        "print(\"=\"*60)\n",
        "print(results_df[['Model', 'R2', 'RMSE', 'CV_R2_Mean']])\n",
        "\n",
        "# Analysis & Insights\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYSIS & INSIGHTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Feature Importance for tree-based models\n",
        "print(\"\\nFeature Importance (Random Forest):\")\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': rf.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(feature_importance)\n",
        "\n",
        "print(\"\\nFeature Importance (Gradient Boosting):\")\n",
        "gb_feature_importance = pd.DataFrame({\n",
        "    'Feature': features,\n",
        "    'Importance': gb.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(gb_feature_importance)\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(15, 10))\n",
        "\n",
        "# 1. Model Comparison\n",
        "plt.subplot(2, 2, 1)\n",
        "models = results_df['Model']\n",
        "r2_scores = results_df['R2']\n",
        "plt.bar(models, r2_scores)\n",
        "plt.title('R² Scores by Model')\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('R² Score')\n",
        "\n",
        "# 2. Feature Importance\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
        "plt.title('Random Forest Feature Importance')\n",
        "plt.xlabel('Importance')\n",
        "\n",
        "# 3. Actual vs Predicted for best model\n",
        "best_model_idx = results_df['R2'].idxmax()\n",
        "best_model_name = results_df.loc[best_model_idx, 'Model']\n",
        "\n",
        "if best_model_name == \"Linear Regression\":\n",
        "    y_pred_best = y_pred_lr\n",
        "elif best_model_name == \"Ridge Regression\":\n",
        "    y_pred_best = y_pred_ridge\n",
        "elif best_model_name == \"Random Forest\":\n",
        "    y_pred_best = y_pred_rf\n",
        "elif best_model_name == \"Gradient Boosting\":\n",
        "    y_pred_best = y_pred_gb\n",
        "else:\n",
        "    y_pred_best = y_pred_svr\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.scatter(y_test, y_pred_best, alpha=0.5)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
        "plt.xlabel('Actual Obesity Rate')\n",
        "plt.ylabel('Predicted Obesity Rate')\n",
        "plt.title(f'Actual vs Predicted ({best_model_name})')\n",
        "\n",
        "# 4. Residual Plot\n",
        "plt.subplot(2, 2, 4)\n",
        "residuals = y_test - y_pred_best\n",
        "plt.scatter(y_pred_best, residuals, alpha=0.5)\n",
        "plt.axhline(y=0, color='r', linestyle='--')\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Additional Insights\n",
        "print(f\"\\nBest performing model: {best_model_name}\")\n",
        "print(f\"Best R² Score: {results_df.loc[best_model_idx, 'R2']:.4f}\")\n",
        "\n",
        "# Analyze the relationship between key features and obesity rate\n",
        "print(\"\\nKey Insights:\")\n",
        "print(\"1. The most important features for predicting obesity rates are:\")\n",
        "for i, row in feature_importance.head(3).iterrows():\n",
        "    print(f\"   - {row['Feature']}: {row['Importance']:.4f}\")\n",
        "\n",
        "print(\"\\n2. Model performance indicates that tree-based methods (Random Forest and Gradient Boosting)\")\n",
        "print(\"   generally perform better than linear models for this dataset.\")\n",
        "\n",
        "print(\"\\n3. The relatively low R² scores suggest that additional features or more sophisticated\")\n",
        "print(\"   feature engineering might be needed to better predict obesity rates.\")\n",
        "\n",
        "# Additional analysis: Yearly trends\n",
        "yearly_obesity = obesity_data.groupby('YearStart')['Data_Value'].mean()\n",
        "print(f\"\\nYearly Obesity Rate Trends:\\n{yearly_obesity}\")\n",
        "\n",
        "# State-wise analysis\n",
        "state_obesity = obesity_data.groupby('LocationDesc')['Data_Value'].mean().sort_values(ascending=False)\n",
        "print(f\"\\nTop 5 States with Highest Obesity Rates:\\n{state_obesity.head(5)}\")\n",
        "print(f\"\\nBottom 5 States with Lowest Obesity Rates:\\n{state_obesity.tail(5)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G6q28XbMRFoX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}